{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a8382e88",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/heavenly/opt/anaconda3/envs/mqed/bin/python'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "sys.executable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e7947ef",
   "metadata": {},
   "source": [
    "# How to use ChatGPT API"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1f8abb0",
   "metadata": {},
   "source": [
    "### Prerequisite condition for the use of ChatGPT API"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c371e56a",
   "metadata": {},
   "source": [
    "<h3>1. Get your own ChatGPT API Key</h3>\n",
    "If you would like to get your own API key, follow the link <a href=\"https://www.howtogeek.com/885918/how-to-get-an-openai-api-key/\">click</a>\n",
    "<h3>2. Install virtual environment(anaconda)</h3>\n",
    "Anaconda is a software to support virtual environment for R and Python. Once you install this programme, you don't need to install Python.<br>\n",
    "follow the link <a href=\"https://www.anaconda.com/download/\">click</a>\n",
    "<h3>3. create and activate virtual environment</h3>\n",
    "Open up your terminal (or command window) and type this command:<br>\n",
    "<b>conda create -n mqed python=3.11</b><br>\n",
    "<b>conda activate mqed</b>\n",
    "<b>pip install -r requirements.txt</b><br>\n",
    "you can download requirements.txt from the git repository<br>\n",
    "<p></p>\n",
    "If you need to install any specific library, open up your terminal in the OS and type this command:<br>\n",
    "<b>pip install <i>package_name</i></b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "58ab5b15",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import re\n",
    "import json\n",
    "import openai\n",
    "from openai._client import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "client = OpenAI(api_key = os.getenv('OPENAI_API_KEY'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2010ad33",
   "metadata": {},
   "outputs": [],
   "source": [
    "TEMPERATURE = 0.0\n",
    "MAX_TOKENS = 4096\n",
    "MODEL = 'gpt-3.5-turbo-1106'\n",
    "context = [] # 리스트"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ac1ef78",
   "metadata": {},
   "source": [
    "## Chatbot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aa6312c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_tokens(items):\n",
    "    cnt = 0\n",
    "\n",
    "    if items is None:\n",
    "        return cnt\n",
    "\n",
    "    for item in items:\n",
    "        cnt += len(item['content'])\n",
    "\n",
    "    return cnt\n",
    "\n",
    "def conversate():\n",
    "    while(1):\n",
    "        message = input('Chat:')\n",
    "        message = message.strip()\n",
    "    \n",
    "        if message == '':\n",
    "            print('Input your talk.')\n",
    "            continue\n",
    "        elif message == 'exit':\n",
    "            break\n",
    "    \n",
    "        # Examine if the size of check is over the maximum tokens\n",
    "        total_cnt = check_tokens(context) + len(message)\n",
    "\n",
    "        if total_cnt >= MAX_TOKENS:\n",
    "            context.clear()\n",
    "            print('context cleared.')\n",
    "\n",
    "        # Setup up for message to call ChatGPT\n",
    "        if len(context) == 0:\n",
    "            context.append({\"role\": \"system\", \"content\": \"You are a helpful assistant.\"})\n",
    "            context.append({\"role\": \"user\", \"content\": message})\n",
    "        else:\n",
    "            context.append({\"role\": \"user\", \"content\": message})\n",
    "\n",
    "        response = client.chat.completions.create(model=MODEL, messages=context, temperature=TEMPERATURE)\n",
    "        answer = response.choices[0].message.content\n",
    "        print(f\"AI: {answer}\")\n",
    "        #codes = markdown.markdown(answer, extensions=['fenced_code', 'codehilite'])\n",
    "        context.append({'role': 'assistant', 'content': answer})\n",
    "\n",
    "        if check_tokens(context) >= MAX_TOKENS:\n",
    "            context.clear()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6d5c70bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Chat: hi, dear\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AI: Hello! How can I assist you today?\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Chat: get me the code to sum from 1 to 10 based on python.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AI: Certainly! You can use the following Python code to sum the numbers from 1 to 10:\n",
      "\n",
      "```python\n",
      "total = 0\n",
      "for i in range(1, 11):\n",
      "    total += i\n",
      "\n",
      "print(\"The sum of numbers from 1 to 10 is:\", total)\n",
      "```\n",
      "\n",
      "When you run this code, it will output the sum of the numbers from 1 to 10, which is 55.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Chat: what about 100?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AI: You can modify the code to sum the numbers from 1 to 100 by changing the range in the for loop. Here's the updated Python code:\n",
      "\n",
      "```python\n",
      "total = 0\n",
      "for i in range(1, 101):\n",
      "    total += i\n",
      "\n",
      "print(\"The sum of numbers from 1 to 100 is:\", total)\n",
      "```\n",
      "\n",
      "When you run this code, it will output the sum of the numbers from 1 to 100.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Chat: tell me the result\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AI: The sum of the numbers from 1 to 100 is 5050.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Chat: bye\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AI: Goodbye! If you have any more questions in the future, feel free to ask. Have a great day!\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Chat: exit\n"
     ]
    }
   ],
   "source": [
    "conversate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4ce03015",
   "metadata": {},
   "outputs": [],
   "source": [
    "def single_ask(message):\n",
    "    message = message.strip()\n",
    "\n",
    "    if message == '':\n",
    "        print('대화 내용을 입력하세요.')\n",
    "    elif message == 'exit':\n",
    "        return\n",
    "\n",
    "    # 대화 맥락을 고려하여 전체 최대 토큰을 초과하는지 체크하도록 한다.\n",
    "    total_cnt = check_tokens(context) + len(message)\n",
    "\n",
    "    if total_cnt >= MAX_TOKENS:\n",
    "        context.clear()\n",
    "        print('context cleared.')\n",
    "\n",
    "    # ChatGPT 대화를 위한 메시지 형태 설정하기\n",
    "    if len(context) == 0:\n",
    "        context.append({\"role\": \"system\", \"content\": \"You are a helpful assistant.\"})\n",
    "        context.append({\"role\": \"user\", \"content\": message})\n",
    "    else:\n",
    "        context.append({\"role\": \"user\", \"content\": message})\n",
    "\n",
    "    response = client.chat.completions.create(model=MODEL, messages=context, temperature=TEMPERATURE)\n",
    "    answer = response.choices[0].message.content\n",
    "    print(f\"AI: {answer}\")\n",
    "    \n",
    "    context.append({'role': 'assistant', 'content': answer})\n",
    "\n",
    "    if check_tokens(context) >= MAX_TOKENS:\n",
    "        context.clear()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3926a7fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AI: Sure! Here are the prime numbers from 1 to 100:\n",
      "\n",
      "2, 3, 5, 7, 11, 13, 17, 19, 23, 29, 31, 37, 41, 43, 47, 53, 59, 61, 67, 71, 73, 79, 83, 89, 97\n",
      "\n",
      "These are the numbers that are only divisible by 1 and themselves.\n"
     ]
    }
   ],
   "source": [
    "single_ask(\"list prime numbers from 1 to 100.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a9bf002c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def debate():\n",
    "    while(1):\n",
    "        message = input('Human: ')\n",
    "        message = message.strip()\n",
    "    \n",
    "        if message == '':\n",
    "            print('Input your text.')\n",
    "            continue\n",
    "        elif message == 'exit':\n",
    "            break\n",
    "    \n",
    "        # 대화 맥락을 고려하여 전체 최대 토큰을 초과하는지 체크하도록 한다.\n",
    "        total_cnt = check_tokens(context) + len(message)\n",
    "\n",
    "        if total_cnt >= MAX_TOKENS:\n",
    "            context.clear()\n",
    "            print('context cleared.')\n",
    "\n",
    "        # ChatGPT 대화를 위한 메시지 형태 설정하기\n",
    "        if len(context) == 0:\n",
    "            context.append({\"role\": \"system\", \"content\": \"You are an excellent debate advisor. Let's have a debate about the decommissioning of nuclear power plants. You will argue against my opinion(disagree).\"})\n",
    "            context.append({\"role\": \"user\", \"content\": message})\n",
    "        else:\n",
    "            context.append({\"role\": \"user\", \"content\": message})\n",
    "\n",
    "        response = client.chat.completions.create(model=MODEL, messages=context, temperature=TEMPERATURE)\n",
    "        answer = response.choices[0].message.content\n",
    "        print(f\"AI: {answer}\")\n",
    "        \n",
    "        context.append({'role': 'assistant', 'content': answer})\n",
    "\n",
    "        if check_tokens(context) >= MAX_TOKENS:\n",
    "            context.clear()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "22dc33e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Human:  I don't agree to decommissioning of nuclear power plants. Without nuclear power plants, we cannot generate electricity for societies.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AI: I understand your perspective. Nuclear power plants have been a significant source of electricity generation for many societies, providing a reliable and consistent supply of energy. The decision to decommission nuclear power plants involves careful consideration of the balance between energy needs, safety, environmental impact, and long-term sustainability.\n",
      "\n",
      "It's important to continue exploring and investing in a diverse range of energy sources to meet the growing demand for electricity. This may include advancements in renewable energy technologies, energy efficiency measures, and potentially new developments in nuclear energy that address safety and waste management concerns.\n",
      "\n",
      "Ultimately, the transition away from nuclear power plants would require a comprehensive and thoughtful approach to ensure that the energy needs of societies are met while also addressing safety and environmental considerations. It's a complex issue that requires careful evaluation and planning.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Human:  There's no better alternative than nuclear power plants. Thermal plants produces air pollutions and green energy like solar plants are ineffective to generate electricity.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AI: While nuclear power plants have been a significant source of electricity generation, it's important to consider the full range of energy options available. Renewable energy sources such as solar, wind, and hydroelectric power have made significant advancements in recent years and are increasingly being used to generate electricity on a large scale.\n",
      "\n",
      "Solar power, in particular, has become more efficient and cost-effective, and advancements in energy storage technology have addressed some of the challenges associated with intermittent generation. Wind power and hydroelectric power also offer reliable and sustainable sources of energy.\n",
      "\n",
      "Additionally, advancements in energy efficiency and smart grid technologies can help reduce overall energy demand and improve the effectiveness of renewable energy sources.\n",
      "\n",
      "While each energy source has its own advantages and challenges, a diverse energy portfolio that includes a mix of renewable energy sources and potentially advanced nuclear technologies can help meet the demand for electricity while addressing environmental and sustainability concerns.\n",
      "\n",
      "It's important to continue evaluating and investing in a range of energy options to ensure a reliable, sustainable, and environmentally responsible energy future.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Human:  exit\n"
     ]
    }
   ],
   "source": [
    "debate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b1951c5",
   "metadata": {},
   "source": [
    "## How to use ChatGPT for textual data analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a3c98db6-8a33-4102-a937-29b38d71f00e",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = '''\n",
    "This is student's text.\n",
    "Extract the elements based on Toulmin's Argumentation Pattern that are present in the text.\n",
    "Extract claim, rebuttal, data, warrant, backing and qualifier from the text.\n",
    "If any of elements are not presented in the text, mark it as 'None.\n",
    "'''\n",
    "txt_def = '''\n",
    "Each element signifies the following:\n",
    "Claim: Assertions about what exists or values that people hold.\n",
    "Data: Statements that are used as evidence to support the claim.\n",
    "Warrant: Statements that explain the relationship of the data to the claim.\n",
    "Backing: Underlying assumptions that are often not made explicit.\n",
    "Qualifier: Special conditions under which the claim holds true.\n",
    "Rebuttal: Statements that contradict either the data, warrant, backing, or qualifier of an argument.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "015e691f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inspect_data(text):\n",
    "    is_first = True\n",
    "    \n",
    "    try:\n",
    "        text = text.strip()\n",
    "        print('Original:', text)\n",
    "        query_msg = query + '\\nText:' + text\n",
    "\n",
    "        # 메시지 설정하기\n",
    "        messages = [\n",
    "                {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "                {\"role\": \"user\", \"content\": query_msg}\n",
    "        ]\n",
    "\n",
    "        # ChatGPT API 호출하기\n",
    "        response = client.chat.completions.create(model=MODEL, messages=messages, temperature=TEMPERATURE)\n",
    "        answer = response.choices[0].message.content\n",
    "        answer = answer.strip()\n",
    "\n",
    "        print(answer)\n",
    "        \n",
    "        return answer\n",
    "        \n",
    "    except openai.APIError as e:\n",
    "        #Handle API error here, e.g. retry or log\n",
    "        print(f\"OpenAI API returned an API Error: {e}\")\n",
    "        return 'Error'\n",
    "        \n",
    "    except openai.APIConnectionError as e:\n",
    "        #Handle connection error here\n",
    "        print(f\"Failed to connect to OpenAI API: {e}\")\n",
    "        return 'Error'\n",
    "        \n",
    "    except openai.RateLimitError as e:\n",
    "        #Handle rate limit error (we recommend using exponential backoff)\n",
    "        print(f\"OpenAI API request exceeded rate limit: {e}\")\n",
    "        return 'Error'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b51ce3d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "sents = '''\n",
    "I can compare the cell to a factory because cells also have their own functions and what or must to do just like in factory.\n",
    "If there's a one cell or one thing that is not functioning, it will not work.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6bec6315",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: I can compare the cell to a factory because cells also have their own functions and what or must to do just like in factory.\n",
      "If there's a one cell or one thing that is not functioning, it will not work.\n",
      "Claim: The cell can be compared to a factory because cells have their own functions and tasks to perform, similar to a factory.\n",
      "\n",
      "Data: None provided explicitly, but the comparison between the cell and a factory is implied.\n",
      "\n",
      "Warrant: The warrant is not explicitly stated, but it can be inferred that the warrant is based on the similarity of functions and tasks between cells and a factory.\n",
      "\n",
      "Backing: None provided in the text.\n",
      "\n",
      "Qualifier: None provided in the text.\n",
      "\n",
      "Rebuttal: None provided in the text.\n"
     ]
    }
   ],
   "source": [
    "result = inspect_data(sents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45e03383-8dfa-4b3a-bb70-94d575b2b0f1",
   "metadata": {},
   "source": [
    "### Using Function Calling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1fcca729-4b79-4a18-b5ce-61a53f86a78a",
   "metadata": {},
   "outputs": [],
   "source": [
    "functions = [\n",
    "    {\n",
    "        \"name\" : \"extract_element\",\n",
    "        \"description\": \"Extract the elements of argumentation from the text.\",\n",
    "        \"parameters\": {\n",
    "            \"type\": \"object\", \n",
    "            \"properties\": {\n",
    "                \"Claim\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"description\": \"Find claim from the text.\"\n",
    "                },\n",
    "                \"Data\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"description\": \"Find data from the text.\"\n",
    "                },\n",
    "                \"Warrant\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"description\": \"Find warrant from the text.\"\n",
    "                },\n",
    "                \"Backing\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"description\": \"Find backing from the text.\"\n",
    "                },\n",
    "                \"Qualifier\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"description\": \"Find qualifier from the text.\"\n",
    "                },\n",
    "                \"Rebuttal\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"description\": \"Find rebuttal from the text.\"\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "83badabc-c5eb-4e49-a294-b469c6b3151e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inspect_data(text):\n",
    "    \n",
    "    start_time = time.time()\n",
    "    sent = text\n",
    "    sent = sent.replace('\\n', ' ').strip()\n",
    "\n",
    "    query_msg = query + '\\nText:' + sent\n",
    "    #query_msg = query + txt_def + '\\nText:' + sent\n",
    "\n",
    "    messages = [\n",
    "            {\"role\": \"system\", \"content\": \"You are an excellent assistant that analyzes element of argumentation from text.\"},\n",
    "            {\"role\": \"user\", \"content\": query_msg}\n",
    "    ]\n",
    "        \n",
    "    response = client.chat.completions.create(model=MODEL,\n",
    "                                              messages=messages,\n",
    "                                              temperature=TEMPERATURE,\n",
    "                                              functions=functions, \n",
    "                                              function_call = {\"name\": functions[0][\"name\"]})\n",
    "    answer = response.choices[0].message.function_call.arguments\n",
    "    answer = json.loads(answer)\n",
    "\n",
    "    arguments = {}\n",
    "\n",
    "    arguments['Answer'] = sent\n",
    "    arguments['Claim'] = answer.get('Claim', '')\n",
    "    arguments['Data'] = answer.get('Data', '')\n",
    "    arguments['Warrant'] = answer.get('Warrant', '')\n",
    "    arguments['Backing'] = answer.get('Backing', '')\n",
    "    arguments['Qualifier'] = answer.get('Qualifier', '')\n",
    "    arguments['Rebuttal'] = answer.get('Rebuttal', '')\n",
    "    \n",
    "    print('Elapsed time:', \"{:.2f}\".format(time.time() - start_time))\n",
    "    \n",
    "    return arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "6c4d49e3-518b-4efb-9524-746c1144b0c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed time: 1.45\n"
     ]
    }
   ],
   "source": [
    "result_dict = inspect_data(sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "f96d7d65-b97a-470c-aa40-741dd30859cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Answer': \"I can compare the cell to a factory because cells also have their own functions and what or must to do just like in factory. If there's a one cell or one thing that is not functioning, it will not work.\",\n",
       " 'Claim': 'I can compare the cell to a factory because cells also have their own functions and what or must to do just like in factory.',\n",
       " 'Data': \"If there's a one cell or one thing that is not functioning, it will not work.\",\n",
       " 'Warrant': '',\n",
       " 'Backing': '',\n",
       " 'Qualifier': '',\n",
       " 'Rebuttal': ''}"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aaab5aa",
   "metadata": {},
   "source": [
    "## Read single PDF and ask questions about the file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "525b1f23-da19-4fa3-a834-3e8a45184771",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pypdf2 tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "c022101f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PyPDF2 import PdfReader\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain import OpenAI\n",
    "from langchain.chains import AnalyzeDocumentChain\n",
    "from langchain.chains.summarize import load_summarize_chain\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain.chat_models import ChatOpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "ab2eda6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 내가 읽고 싶은 파일의 위치를 지정하도록 한다.\n",
    "reader = PdfReader(\"personal/the scientific method as myth and ideal.pdf\")\n",
    "\n",
    "raw_text = \"\"\n",
    "\n",
    "for i, page in enumerate(reader.pages):\n",
    "    text = page.extract_text()\n",
    "    if text:\n",
    "        raw_text += text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "6364dc2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = OpenAI(temperature=0, openai_api_key = os.getenv('OPENAI_API_KEY'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "df8f0eb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_chain = load_summarize_chain(llm, chain_type=\"map_reduce\")\n",
    "summarize_document_chain = AnalyzeDocumentChain(combine_docs_chain=summary_chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "e722302f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' This article examines the shortcomings of the popularly-taught \"Scientific Method\" and provides corrections to common misconceptions. It looks at the lack of consensus on the method, with different views from Aristotle, Bacon, and Newton, and discusses the classical inductivism view. It also considers the implications for introductory pedagogical contexts, the criticisms of classical inductivism, the importance of social dynamics for the scientific enterprise, the role of data-mining, the differences between scientific hypotheses and questions, the difference between science and common sense, the aims, values, and virtues of scientific activity, and the contributions of students, audience members, and other sources of support. The author argues that the myth of the scientific method should be discarded, but the ideals associated with it should be promoted.'"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summarize_document_chain.run(raw_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c93ebfde",
   "metadata": {},
   "source": [
    "## Analyse multiple douments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "3bb447d9-7294-4095-93c3-9cfe3832f415",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pypdf\n",
      "  Downloading pypdf-3.17.2-py3-none-any.whl.metadata (7.5 kB)\n",
      "Downloading pypdf-3.17.2-py3-none-any.whl (277 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m277.9/277.9 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: pypdf\n",
      "Successfully installed pypdf-3.17.2\n"
     ]
    }
   ],
   "source": [
    "!pip install pypdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "be4c82b7-59a2-4fff-b04a-8fb3f666d7ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import PyPDF2\n",
    "\n",
    "source_folder = 'personal'\n",
    "\n",
    "# 폴더 내 모든 파일 리스트 가져오기\n",
    "file_list = []\n",
    "for root, dirs, files in os.walk(source_folder):\n",
    "    for file in files:\n",
    "        if file.endswith('.pdf'):\n",
    "            file_list.append(os.path.join(root, file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "6e603106-a5ea-464e-8039-ac0ed24ecb6d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['personal/dynamic social representations.pdf',\n",
       " 'personal/the scientific method as myth and ideal.pdf']"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "f96f7c12-2cc1-4459-995a-63c182539cc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is a file_list for single PDF\n",
    "file_list = [file_list[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "e43fb832-14aa-428a-962d-d597fc1b0077",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "personal/dynamic social representations.pdf Loaded:::::\n"
     ]
    }
   ],
   "source": [
    "documents = []\n",
    "\n",
    "for name in file_list:\n",
    "    print(f\"{name} Loaded:::::\")\n",
    "    reader = PyPDFLoader(name)\n",
    "    documents.extend(reader.load())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "4f9d7923-3376-4943-a2bd-c1ad7689059b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You have 14 document(s) in your data\n"
     ]
    }
   ],
   "source": [
    "print (f'You have {len(documents)} document(s) in your data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "179a0e04-570b-4698-8d29-6aea1834b830",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14\n"
     ]
    }
   ],
   "source": [
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "\n",
    "text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=100) #chunk overlap seems to work better\n",
    "documents2 = text_splitter.split_documents(documents)\n",
    "print(len(documents2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "a481d180",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = OpenAIEmbeddings(model='text-embedding-ada-002', max_retries=20, openai_api_key=os.getenv('OPENAI_API_KEY'))\n",
    "vectorstore = Chroma.from_documents(documents, embeddings) # 벡터화된 DB\n",
    "model = ChatOpenAI(temperature=TEMPERATURE, model_name=MODEL, openai_api_key=os.getenv('OPENAI_API_KEY'))\n",
    "qa = ConversationalRetrievalChain.from_llm(model, vectorstore.as_retriever())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "a87e535d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ask(question):\n",
    "    result = qa({\"question\":question, \"chat_history\":[]})\n",
    "    \n",
    "    return result['answer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "f26d0fe8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The key argument of the article seems to be the analysis of the evolution of mentions of categories of collectives in interviews and newspaper articles during different waves of the H1N1 pandemic. The article discusses how mentions of distant categories of collectives decreased over time, while mentions of local collectives increased, and it also explores the themes associated with key categories of collectives, such as victims, heroes, and villains. The article also highlights the agenda-setting effects and differences between mentions in interviews and newspaper articles.'"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ask('What is the key argument of the article?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "232e2855-bd4e-4f62-aa46-e584fcfe5607",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I don't have enough information to answer that question.\""
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ask('What is most important findng from the research?')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82a1fec7-081f-4046-a79c-17e8750ed65c",
   "metadata": {},
   "source": [
    "### Save the vector DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "1ca89dbb-a0f3-4602-a72d-27d2e1377bcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import Chroma\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from chromadb.config import Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "d2889b9d-1ecd-4ee5-8763-6bc77aed8710",
   "metadata": {},
   "outputs": [],
   "source": [
    "persist_directory = 'sample_db'\n",
    "embeddings = OpenAIEmbeddings(model='text-embedding-ada-002', openai_api_key=os.getenv('OPENAI_API_KEY'), max_retries=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "5e84934a-454c-4fbd-913a-78abc927b9f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "f061a808-3d2e-4337-a642-37d7b0d7ae82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finishing 10\n",
      "finishing 20\n"
     ]
    }
   ],
   "source": [
    "while i < len(documents):\n",
    "    size = i + 10\n",
    "    docs = documents[i:size]\n",
    "    vectorstore = Chroma.from_documents(docs, embeddings, persist_directory=persist_directory)\n",
    "\n",
    "    vectorstore.persist()   \n",
    "    i = i + 10\n",
    "    print('finishing', i)\n",
    "    time.sleep(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a8c3772-38bb-449c-bfa8-5fee14792815",
   "metadata": {},
   "source": [
    "# Build up your own Conversational AI Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "44573738-e1d1-4b5e-808c-bb3c1dadb1e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import Chroma\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from dotenv import load_dotenv\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.schema import AIMessage, HumanMessage, SystemMessage\n",
    "import langchain\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "1543bd6f-05cb-4540-b7c8-90c13982db1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "OPENAI_API_KEY = os.getenv('OPENAI_API_KEY')\n",
    "\n",
    "persist_directory = 'sample_db'\n",
    "embeddings = OpenAIEmbeddings(model='text-embedding-ada-002', openai_api_key=OPENAI_API_KEY, max_retries=20)\n",
    "vectorstore = Chroma(persist_directory=persist_directory, embedding_function=embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "210ac255-14bd-4055-9fbf-58bde574d0ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ChatOpenAI(temperature=0, model_name='gpt-3.5-turbo-1106')\n",
    "qa = ConversationalRetrievalChain.from_llm(model, \n",
    "                                           vectorstore.as_retriever(search_kwargs={\"k\": 5}), \n",
    "                                           return_source_documents=True)\n",
    "memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4c0f36e-a333-44e8-bfe7-8b40eb293229",
   "metadata": {},
   "outputs": [],
   "source": [
    "#qa = ConversationalRetrievalChain.from_llm(model, vectorstore.as_retriever(), memory=memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "6093d2cb-a0be-46a2-a2d3-58c9559a3806",
   "metadata": {},
   "outputs": [],
   "source": [
    "def manage_history(lists):    \n",
    "    tot_size = 0\n",
    "    if len(lists) >= 1:\n",
    "        for item in lists:\n",
    "            tot_size += len(item.content)\n",
    "\n",
    "        if tot_size >= 3000:\n",
    "            lists = lists[2:]  # chat_history 전역 변수를 수정\n",
    "        \n",
    "        return lists\n",
    "    else:\n",
    "        return lists\n",
    "        \n",
    "def questioning(lists, query, show_ref=True):\n",
    "    lists = manage_history(lists)\n",
    "    \n",
    "    result = qa({\"question\": query, \"chat_history\": lists})\n",
    "    print(result['answer']) # 응답 출력\n",
    "        \n",
    "    #Reference 출력\n",
    "    refs = []\n",
    "    if show_ref: print('Reference:')\n",
    "        \n",
    "    for item in result['source_documents']:\n",
    "        filename_with_extension = os.path.basename(item.metadata['source'])\n",
    "        filename = os.path.splitext(filename_with_extension)[0]\n",
    "        if show_ref: print(filename)\n",
    "        refs.append(filename)\n",
    "        \n",
    "    lists.append(HumanMessage(content=query, additional_kwargs={}, example=False))\n",
    "    lists.append(AIMessage(content=result['answer'], additional_kwargs={'source':refs}, example=False))\n",
    "        \n",
    "    return lists, refs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "54113719-6e5f-4a98-ab70-ea971ee10f1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_history = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "ef537532-b2d0-4b28-bbf8-a9661581052c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Influenza is caused by the influenza virus. There are different strains of the virus, such as H1N1, which can cause outbreaks and pandemics. The specific cause of each outbreak can vary, but it is generally attributed to the spread of the virus through respiratory droplets from infected individuals.\n",
      "Reference:\n",
      "dynamic social representations\n",
      "dynamic social representations\n",
      "dynamic social representations\n",
      "dynamic social representations\n",
      "dynamic social representations\n"
     ]
    }
   ],
   "source": [
    "chat_history, ref = questioning(chat_history, \"What is the main cause of influenza?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "a8cc6f63-48b6-434d-a2d5-84fe05344b77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Some ways to prevent catching the flu include getting vaccinated, practicing good hygiene by washing your hands frequently, avoiding close contact with sick individuals, covering your mouth and nose when coughing or sneezing, and cleaning and disinfecting frequently touched surfaces. Additionally, it's important to maintain a healthy lifestyle with a balanced diet, regular exercise, and adequate sleep to support your immune system.\n",
      "Reference:\n",
      "dynamic social representations\n",
      "dynamic social representations\n",
      "dynamic social representations\n",
      "dynamic social representations\n",
      "dynamic social representations\n"
     ]
    }
   ],
   "source": [
    "chat_history = questioning(chat_history, \"How can we prevent from catching a flu?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f1315b7-9c4c-4cdc-9ab4-693d8efbf179",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
